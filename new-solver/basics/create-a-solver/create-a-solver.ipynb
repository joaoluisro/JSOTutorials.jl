{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a JSO compatible solver\n### João Okimoto\n\n![](../../jso.png)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg\npkg\"activate .\"\nif false\n pkg\"add NLPModels\"\n pkg\"add SolverTools\"\n pkg\"add SolverBenchmark\"\n pkg\"add Plots\"\n pkg\"add LinearAlgebra\"\n pkg\"add JSOSolvers\"\nend\npkg\"instantiate\"\npkg\"status\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial covers the basics on how to create a solver compatible with the\nJuliaSmoothOptimizers format and do basic benchmarking.\nLet's start off with a simple implementation of Newton's method:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using NLPModels, LinearAlgebra, SolverTools\n\nfunction newton(nlp :: AbstractNLPModel;\n                x :: AbstractVector = copy(nlp.meta.x0),\n                max_tol :: Real = √eps(eltype(x)),\n                max_time :: Float64 = 30.0,\n                max_iter :: Int = 100)\n\n  T = eltype(x)\n  k = 0\n  el_time = 0.0\n  start_time = time()\n  tired = el_time > max_time || k ≥ max_iter\n  optimal = norm(grad(nlp, x)) < max_tol\n\n  @info log_header([:iter, :f, :nrmgrad], [Int, T, T], hdr_override=Dict(:f => \"f(x)\", :nrmgrad => \"‖∇f(x)‖\"))\n\n  while !(optimal || tired)\n\n    fx = obj(nlp, x)\n    ∇fx = grad(nlp, x)\n    nrmgrad = norm(∇fx)\n    ∇²fx = Symmetric(hess(nlp, x), :L)\n\n    @info log_row(Any[k, fx, nrmgrad])\n\n    d = isposdef(∇²fx) ? ∇²fx\\ -∇fx : -∇fx\n    t = one(T)\n    α = 0.5\n\n    while obj(nlp, x + t*d) > fx + α * ∇fx' * t * d\n      t *= 0.5\n    end\n\n    x += t*d\n\n    k += 1\n    el_time = time() - start_time\n    tired = el_time > max_time || k ≥ max_iter\n    optimal = nrmgrad < max_tol\n  end\n\n  if optimal\n    status =:first_order\n  elseif tired\n    if k ≥ max_iter\n      status =:max_iter\n    else\n      status =:max_time\n    end\n  else\n    status =:unknown\n  end\n\n  return GenericExecutionStats(status, nlp, solution=x, objective=obj(nlp, x),\n                               iter = k, elapsed_time = el_time)\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the example above, our ```newton``` solver has a few important\nfeatures:\n- It receives the **JSO Standard input**, an **AbtractNLPModel**, part of the [NLPModels](https://github.com/JuliaSmoothOptimizers/NLPModels.jl) package.\n- Derivatives are computed according to the **AbstractNLPModel** implementation with the ``hess`` and ``grad`` functions.\n- It returns the **JSO Standard output**, a **GenericExecutionStats**, part of the [SolverTools](https://github.com/JuliaSmoothOptimizers/SolverTools.jl) package.\n\nBoth input and output are important when interacting with other JSO packages,\nsince they serve as a way of solving a optimization problem while also keeping\ntrack of relevant information. An **AbstractNLPModel** is the main structure of\nthe problem, as it contains details regarding its description (i.e function to\nbe minimized, starting point, number of variables, etc.), while a\n**GenericExecutionStats** is responsible for carrying information over how the\nproblem was solved (i.e number of iterations, time elapsed, number of function\nevaluations, etc.).\n\nAnother important thing to highlight, is that we'll be using the **ADNLPModel**\nstruct to run our solver. By default, the package responsible for calculating\nderivatives is [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) since\nthat's the built in method for differentation in this **AbstractNLPModel**\nimplementation.\n\nWhen creating a new JSO compatible solver, it's also important to manipulate\nthe output data to match that of **GenericExecutionStats**. The ``status``\nattribute tells us about how our solver did given our input problem. It is\npossible that the number of iterations has exceed the max limit, or the time it\ntook for it to solve was greater than allowed. Either way, the ``status`` is an\nimportant attribute when benchmarking and testing, as it allows the user to keep\ntrack of how a solver behaves over a specific problem.\n\nIf we want to run our solver, we simply need to create a optimization problem"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = (x[1]^2 + x[2]^2)^2\n  problem = ADNLPModel(f, [1.0,2.0])\n  output = newton(problem)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "whose information regarding its solution is stored in ``output``."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a solver in hands, we can start to do more interesting things, such as\nbenchmarking and comparing our ```newton``` method to other solvers. Since JSO\ncontains a lot of useful packages, we'll be using a collection of those to make\nour comparison easier. The most important one by far is [SolverBenchmarks](https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl), a\nbenchmarking package designed for manipulating DataFrames that contain statistics\nregarding the execution of a specific sover given a set of problems.\n\nLet us compare our ```newton``` method to a solver contained in the\n[JSOSolvers](https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl) package, namely the ```lbfgs``` one.\nFirstly, we'll need to gather our data regarding each solver."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SolverBenchmark, JSOSolvers\nsolvers = Dict(:newton=>newton, :lbfgs=>lbfgs)\nf1(x) = x[1]^2 + x[2]^2\nf2(x) = (1 - x[1])^2 + 100(x[2] - x[1]^2)^2\nf3(x) = (x[1]^2 + x[2] - 11) + (x[1] + x[2]^2 - 7)^2\nf4(x) = 0.26(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2]\ntest_functions = [f1, f2, f3, f4]\nproblems = (ADNLPModel(i, 2*ones(2),name=\"Problem $i\") for i in test_functions)\nstats = bmark_solvers(solvers, problems)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``bmark_solvers`` function is also part of the **SolverTools** package, and\nruns our solver on a set of problems, in our case ``f1``, ``f2``, ``f3`` and ``f4``.\nLastly, it returns a DataFrame of statistics that we can manipulate using\n**SolverBenchmarks**.\n\nNext, let's create a latex table with the data we just gathered"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "open(\"newton.tex\",\"w\") do io\n\tlatex_table(io,stats[:newton],cols = [:name,:status,:objective,:elapsed_time,:iter])\nend\nopen(\"lbfgs.tex\",\"w\") do io\n\tlatex_table(io,stats[:lbfgs],cols = [:name,:status,:objective,:elapsed_time,:iter])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That will give us a nicely formatted table that we can just plug into our\nlatex code. It's also possible to create the table in the markdown format."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "open(\"newton.md\",\"w\") do io\n\tmarkdown_table(io,stats[:newton],cols = [:name,:status,:objective,:elapsed_time,:iter])\nend\nopen(\"lbfgs.md\",\"w\") do io\n\tmarkdown_table(io,stats[:lbfgs],cols = [:name,:status,:objective,:elapsed_time,:iter])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Either way, our tables should look something like this:\n### LBFGS\n\n```\n|       name |      status | objective | elapsed_time | iter |\n|------------|-------------|-----------|--------------|------|\n| Problem f1 | first_order |   0.0e+00 |      1.2e+00 |    2 |\n| Problem f2 | first_order |   1.3e-16 |      7.0e-04 |   25 |\n| Problem f3 | first_order |  -8.4e+00 |      3.7e-04 |    8 |\n| Problem f4 | first_order |   1.2e-28 |      9.8e-05 |    2 |\n```\n### Newton\n\n```\n|       name |      status | objective | elapsed_time | iter |\n|------------|-------------|-----------|--------------|------|\n| Problem f1 | first_order |   0.0e+00 |      6.1e-01 |    2 |\n| Problem f2 | first_order |   0.0e+00 |      1.5e-01 |   17 |\n| Problem f3 | first_order |  -8.4e+00 |      1.5e-01 |   16 |\n| Problem f4 | first_order |   5.6e-62 |      1.4e-01 |    2 |\n```\nIn the latex format\n\n![](lbfgs.svg)\n\n![](newton.svg)\n\nTo create a performance profile, we use the **Plots** package, along with the desired framework."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nplotly()\nperformance_profile(stats, df->df.elapsed_time)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3.1"
    },
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
