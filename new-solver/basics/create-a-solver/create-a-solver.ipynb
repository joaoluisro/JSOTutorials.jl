{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a JSO compatible solver\n### João Okimoto\n\n![](../../jso-banner.png)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg\npkg\"activate .\"\nif false\n  pkg\"add NLPModels\"\n  pkg\"add SolverTools\"\n  pkg\"add SolverBenchmark\"\n  pkg\"add Plots\"\n  pkg\"add PyPlot\"\n  pkg\"add LinearAlgebra\"\n  pkg\"add JSOSolvers\"\n  pkg\"add Weave\"\nend\npkg\"instantiate\"\npkg\"status\""
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Julia version and JSO packages:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "pkgs = [\"NLPModels\", \"SolverTools\", \"SolverBenchmark\", \"JSOSolvers\"]\n\nusing Pkg\nctx=Pkg.Types.Context()\ndisplay(\"text/html\", \"<img src=\\\"https://img.shields.io/badge/julia-$VERSION-3a5fcc.svg?style=flat-square&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAMAAAAolt3jAAAB+FBMVEUAAAA3lyU3liQ4mCY4mCY3lyU4lyY1liM3mCUskhlSpkIvkx0zlSEeigo5mSc8mio0liKPxYQ/nC5NozxQpUBHoDY3lyQ5mCc3lyY6mSg3lyVPpD9frVBgrVFZqUpEnjNgrVE3lyU8mio8mipWqEZhrVJgrVFfrE9JoTkAVAA3lyXJOjPMPjNZhCowmiNOoz1erE9grVFYqUhCnjFmk2KFYpqUV7KTWLDKOjK8CADORj7GJx3SJyVAmCtKojpOoz1DnzFVeVWVSLj///+UV7GVWbK8GBjPTEPMQTjPTUXQUkrQSEGZUycXmg+WXbKfZ7qgarqbYraSVLCUV7HLPDTKNy7QUEjUYVrVY1zTXFXPRz2UVLmha7upeMCqecGlcb6aYLWfaLrLPjXLPjXSWFDVZF3VY1zVYlvRTkSaWKqlcr6qesGqecGpd8CdZbjo2+7LPTTKOS/QUUnVYlvVY1zUXVbPSD6TV7OibLuqecGqecGmc76aYbaibLvKOC/SWlPMQjrQUEjRVEzPS0PLPDL7WROZX7WgarqibLucY7eTVrCVWLLLOzLGLCLQT0bIMynKOC7FJx3MPjV/Vc+odsCRUa+SVLCDPaWVWLKWWrLJOzPHOTLKPDPLPDPLOzLLPDOUV6+UV7CVWLKVWLKUV7GUWLGPUqv///8iGqajAAAAp3RSTlMAAAAAAAAAAAAAABAZBAAAAABOx9uVFQAAAB/Y////eQAAADv0////pgEAAAAAGtD///9uAAAAAAAAAAcOQbPLfxgNAAAAAAA5sMyGGg1Ht8p6CwAAFMf///94H9j///xiAAAw7////65K+f///5gAABjQ////gibg////bAAAAEfD3JwaAFfK2o0RAAAAAA4aBQAAABEZAwAAAAAAAAAAAAAAAAAAAIvMfRYAAAA6SURBVAjXtcexEUBAFAXAfTM/IDH6uAbUqkItyAQYR26zDeS0UxieBvPVbArjXd9GS295raa/Gmu/A7zfBRgv03cCAAAAAElFTkSuQmCC\\\">\")\nfor p in pkgs\n  uuid=ctx.env.project.deps[p]\n  v=ctx.env.manifest[uuid].version\n  c=string(hash(p) % 0x1000000, base=16)\n  display(\"text/html\", \"<img src=\\\"https://img.shields.io/badge/$p-$v-brightgreen?color=$c\\\">\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n\n\nThis tutorial covers the basics on how to create a solver compatible with the\nJuliaSmoothOptimizers format and how to perform basic benchmarking.\n## Newton's method\nTo illustrate the procedure for creating a solver with the JSO API, i'll be implementing\na Line-Search Newton solver. The method consists of following\nthe direction $$ d_{k} $$ that minimizes $$ m_{k}(p) = f(x_{k}) + g_{k}^{T}p + \\frac{1}{2}p^{T}H_{k}p  $$,\nwhere $$ g_{k} = ∇f(x_{k}) $$ and $$ H_{k} = ∇f^{2}(x_{k}) $$. The direction $$ d $$\ncan then be written as $$ d_{k} = - H_{k}^{-1}g_{k} $$. For the Line-Search strategy\n, we use Backtracking and ask that the Armijo condition be satisfied, that is\n$$ f(x_{k} + td_{k}) < f(x_{k}) + αg_{k}^{T}td_{k}$$ for some $$ α ∈ (0,1) $$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using NLPModels, LinearAlgebra, SolverTools\n\nfunction newton(nlp :: AbstractNLPModel;\n                x :: AbstractVector = copy(nlp.meta.x0),\n                max_tol :: Real = √eps(eltype(x)),\n                max_time :: Float64 = 30.0,\n                max_iter :: Int = 100)\n\n  fx = obj(nlp, x)\n  ∇fx = grad(nlp, x)\n  nrmgrad = norm(∇fx)\n  ∇²fx = Symmetric(hess(nlp, x), :L)\n\n  T = eltype(x)\n  k = 0\n  el_time = 0.0\n  start_time = time()\n  tired = el_time > max_time || k ≥ max_iter\n  optimal = nrmgrad < max_tol\n\n  @info log_header([:iter, :f, :nrmgrad], [Int, T, T], hdr_override=Dict(:f => \"f(x)\", :nrmgrad => \"‖∇f(x)‖\"))\n\n  while !(optimal || tired)\n\n    @info log_row(Any[k, fx, nrmgrad])\n\n    step = -∇²fx\\∇fx\n    d = dot(step, ∇fx) < 0 ? step : -∇fx\n    t = one(T)\n    α = 0.5\n    slope = dot(∇fx, t*d)\n    xt = x + t*d\n    ft = obj(nlp, xt)\n\n    while ft > fx + α * slope\n      t *= 0.5\n      xt = x + t*d\n      ft = obj(nlp, xt)\n      slope =  dot(∇fx, t*d)\n    end\n\n    x += t*d\n\n    fx = obj(nlp, x)\n    ∇fx = grad(nlp, x)\n    nrmgrad = norm(∇fx)\n    ∇²fx = Symmetric(hess(nlp, x), :L)\n\n    k += 1\n    el_time = time() - start_time\n    tired = el_time > max_time || k ≥ max_iter\n    optimal = nrmgrad < max_tol\n  end\n\n  if optimal\n    status =:first_order\n  elseif tired\n    if k ≥ max_iter\n      status =:max_iter\n    else\n      status =:max_time\n    end\n  else\n    status =:unknown\n  end\n\n  return GenericExecutionStats(status, nlp, solution=x, objective=fx,\n                               iter = k, elapsed_time = el_time)\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the example above, our `newton` solver has a few important\nfeatures:\n- It receives the **JSO Standard input**, an **AbtractNLPModel**, part of the [NLPModels](https://github.com/JuliaSmoothOptimizers/NLPModels.jl) package.\n- Derivatives are computed according to the **AbstractNLPModel** API using the `hess` and `grad` functions.\n- It returns the **JSO Standard output**, a **GenericExecutionStats**, part of the [SolverTools](https://github.com/JuliaSmoothOptimizers/SolverTools.jl) package.\n\nBoth input and output are important when interacting with other JSO packages.\nAn **AbstractNLPModel** is the main structure of the problem, as it contains\ndetails regarding its description (i.e. function to be minimized, starting point,\nnumber of variables, etc.), while a **GenericExecutionStats** is responsible for\ncarrying information over how the problem was solved (i.e. number of iterations,\ntime elapsed, number of function evaluations, etc.).\n\nAnother important thing to highlight, is that we'll be using the **ADNLPModel**\nstruct to run our solver. **ADNLPModel** is a kind of **AbstractNLPModel**, that\nuses [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) to compute\nderivatives.\n\nWhen creating a new JSO compatible solver, it's also important to manipulate\nthe output data to match that of **GenericExecutionStats**. The `status`\nattribute tells us about the result of the optimization given our input problem.\nIt is possible that the number of iterations has exceed the maximum limit,\nor the time it took for it to solve was greater than allowed. Either way, the ``status`` is an\nimportant attribute when benchmarking and testing, as it allows the user to keep\ntrack of how a solver behaves over a specific problem. It is possible to list\nall the currently acceptable statuses by running `SolverTools.show_statuses()`"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SolverTools\n\tSolverTools.show_statuses()"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to run our solver, we simply need to create a optimization problem"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f(x) = (x[1]^2 + x[2]^2)^2\nproblem = ADNLPModel(f, [1.0,2.0])\noutput = newton(problem)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information regarding the solution of the problem is stored in `output`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a solver in hands, we can start to do more interesting things, such as\nbenchmarking and comparing our `newton` method to other solvers. Since JSO\ncontains a lot of useful packages, we'll be using a collection of those to make\nour comparison easier. The packages we'll be using are [SolverTools](https://github.com/JuliaSmoothOptimizers/SolverTools.jl)\nand [SolverBenchmarks](https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl).\nThe first is a package that provides tools for developing solvers, and the last,\nis a package that's designed for manipulating DataFrames that contain statistics\nregarding the execution of a specific solver given a set of problems.\n\nLet us compare our `newton` method to a solver contained in the\n[JSOSolvers](https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl) package,\nnamely `lbfgs`, a line-search solver using limited memory BFGS updates.\nFirstly, we'll need to gather the data regarding each solver."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SolverBenchmark, JSOSolvers\nsolvers = Dict(:newton=>newton, :lbfgs=>lbfgs)\nf1(x) = x[1]^2 + x[2]^2\nf2(x) = (1 - x[1])^2 + 100(x[2] - x[1]^2)^2\nf3(x) = x[1]^2 + x[2] - 11 + (x[1] + x[2]^2 - 7)^2\nf4(x) = 0.26 * (x[1]^2 + x[2]^2) - 0.48 * x[1] * x[2]\ntest_functions = [f1, f2, f3, f4]\nproblems = (ADNLPModel(i, 2*ones(2), name=\"Problem $i\") for i in test_functions)\nstats = bmark_solvers(solvers, problems)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `bmark_solvers` function is part of the **SolverTools** package, and\nruns our solver on a set of problems given by `problems`, in our case `f1`, `f2`\n, `f3` and `f4`. Lastly, it returns a dictionary `solver => DataFrame`, where\n`DataFrame` is the execution data of that solver in the problems, that we can manipulate using\n**SolverBenchmarks**.\n\nNext, let's create a latex table with the data we just gathered"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "open(\"newton.tex\", \"w\") do io\n  latex_table(io, stats[:newton], cols = [:name, :status, :objective, :elapsed_time, :iter])\nend\nopen(\"lbfgs.tex\", \"w\") do io\n  latex_table(io, stats[:lbfgs], cols = [:name, :status, :objective, :elapsed_time, :iter])\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That will give us a nicely formatted table that we can just plug into our\nlatex code. It's also possible to create the table in the markdown format."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "markdown_table(stdout, stats[:newton],cols = [:name,:status,:objective,:elapsed_time,:iter])\n\nmarkdown_table(stdout, stats[:lbfgs],cols = [:name,:status,:objective,:elapsed_time,:iter])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the latex format\n\n![](lbfgs.svg)\n\n![](newton.svg)\n\nTo create a performance profile, we use the **Plots** package, along with the\ndesired framework. The profiles are drawn using [BenchmarkProfiles](https://github.com/JuliaSmoothOptimizers/BenchmarkProfiles.jl) through an\ninterface in SolverBenchmark to use `stats`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\npyplot()\nperformance_profile(stats, df->df.elapsed_time)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.3.1"
    },
    "kernelspec": {
      "name": "julia-1.3",
      "display_name": "Julia 1.3.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
